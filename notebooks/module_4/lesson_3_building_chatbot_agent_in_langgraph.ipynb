{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Chatbot Agent in LangGraph\n",
    "\n",
    "In this tutorial, we will build a support chatbot using LangGraph. We will implement this chatbot via a graph workflow using nodes and edges.\n",
    "\n",
    "The chatbot will:\n",
    "\n",
    "- Answer common questions by searching the web.\n",
    "- Use custom tools to enhance its capabilities.\n",
    "- Maintain conversation state across calls.\n",
    "\n",
    "This tutorial builds the foundation for more complex workflows and design patterns later in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "We start by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "from datetime import datetime\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model\n",
    "Initialize the language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Tools\n",
    "We will define two tools:\n",
    "\n",
    "- Tavily Search Tool: To fetch relevant information from the web.\n",
    "- Current Date and Time Tool: Helps the chatbot respond to time-based queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tavily Search Tool\n",
    "tavily_search = TavilySearchResults(max_results=2)\n",
    "\n",
    "# Current Date and Time Tool\n",
    "@tool\n",
    "def get_current_date():\n",
    "    \"\"\"Returns the current date and time. Use this tool first for any time-based queries.\"\"\"\n",
    "    return f\"The current date is: {datetime.now().strftime('%d %B %Y')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of tools for the agent\n",
    "tools = [tavily_search, get_current_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binding Tools to the LLM\n",
    "We bind the tools to the language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the StateGraph\n",
    "We define the state of our chatbot using StateGraph. The State consists of a messages list, which will store the conversation history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Graph Nodes\n",
    "We define a chatbot node, which represents the LLM's response generation. We also define a tool node to handle tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x120c487d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x120c487d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_node = ToolNode(tools=tools)\n",
    "graph_builder.add_node(\"tools\", tool_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Graph Edges\n",
    "We define how the chatbot transitions between nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x120c487d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conditional edge to decide whether to use tools, this is a prebuilt conditional edge\n",
    "graph_builder.add_conditional_edges(\"chatbot\", tools_condition)\n",
    "\n",
    "# If tools are used, return to the chatbot to process the tool output\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "\n",
    "# Set the entry point of the graph\n",
    "graph_builder.set_entry_point(\"chatbot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the Graph\n",
    "We compile the graph to create a runnable chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "### Visualizing the Chatbot's Workflow\n",
    "We can visualize the chatbot's workflow using LangGraph's graph visualization tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "# Visualize the chatbot's workflow\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass  # Visualization requires additional dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Chatbot\n",
    "Now, we can interact with our chatbot.\n",
    "\n",
    "#### Helper Functions\n",
    "We define helper functions to process and render the chatbot's responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def render_markdown(md_string):\n",
    "    display(Markdown(md_string))\n",
    "\n",
    "def process_stream(stream):\n",
    "    for s in stream:\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()\n",
    "    return message\n",
    "\n",
    "def process_query(query, config=None):\n",
    "    inputs = {\"messages\": [(\"user\", query)]}\n",
    "    message = process_stream(graph.stream(inputs, config, stream_mode=\"values\"))\n",
    "    render_markdown(f\"## Answer:\\n{message.content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Chatbot\n",
    "We can now test the chatbot with various queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello! What can you do?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello! I can help with a variety of tasks, including:\n",
      "\n",
      "1. **Answering Questions:** I can provide information on a wide range of topics, including general knowledge, science, history, and more.\n",
      "\n",
      "2. **Current Events:** I can look up the latest news and current events.\n",
      "\n",
      "3. **Time and Date:** I can tell you the current date and time.\n",
      "\n",
      "4. **Search Engines:** I can use search engines to find specific information online.\n",
      "\n",
      "If you have a specific question or task in mind, feel free to ask!\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Answer:\n",
       "Hello! I can help with a variety of tasks, including:\n",
       "\n",
       "1. **Answering Questions:** I can provide information on a wide range of topics, including general knowledge, science, history, and more.\n",
       "\n",
       "2. **Current Events:** I can look up the latest news and current events.\n",
       "\n",
       "3. **Time and Date:** I can tell you the current date and time.\n",
       "\n",
       "4. **Search Engines:** I can use search engines to find specific information online.\n",
       "\n",
       "If you have a specific question or task in mind, feel free to ask!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "process_query(\"Hello! What can you do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the current date?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_current_date (call_CD5gXprURmjpES6STD8Li566)\n",
      " Call ID: call_CD5gXprURmjpES6STD8Li566\n",
      "  Args:\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_current_date\n",
      "\n",
      "The current date is: 15 September 2025\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Today's date is September 15, 2025.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Answer:\n",
       "Today's date is September 15, 2025."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query that requires current date\n",
    "process_query(\"What is the current date?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is LangGraph?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_abRGfMbgRFUbAoyy4eszen2e)\n",
      " Call ID: call_abRGfMbgRFUbAoyy4eszen2e\n",
      "  Args:\n",
      "    query: LangGraph\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"url\": \"https://www.ibm.com/think/topics/langgraph\", \"content\": \"LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. It provides a set of tools and libraries that enable users to create, run and optimize large language models (LLMs) in a scalable and efficient manner. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. [...] Agent systems: LangGraph provides a framework for building agent-based systems, which can be used in applications such as robotics, autonomous vehicles or video games.\\n\\nLLM applications: By using LangGraph’s capabilities, developers can build more sophisticated AI models that learn and improve over time. Norwegian Cruise Line uses LangGraph to compile, construct and refine guest-facing AI solutions. This capability allows for improved and personalized guest experiences. [...] By using a graph-based architecture, LangGraph enables users to scale artificial intelligence workflows without slowing down or sacrificing efficiency. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback. In the world of LLMs, this process is referred to as reflection.\"}, {\"url\": \"https://www.analyticsvidhya.com/blog/2024/07/langgraph-revolutionizing-ai-agent/\", \"content\": \"LangGraph is a library built on top of Langchain that is designed to facilitate the creation of cyclic graphs for large language model (LLM) – based AI agents.\\n It views agent Objective Points about LangGraph and workflows as cyclic graph topologies, allowing for more variable and nuanced agent behaviors than linear execution models. [...] To sum up, LangGraph is a major advancement in the development of AI agents. It enables developers to push the limits of what’s possible with AI agents by eliminating the shortcomings of earlier systems and offering a flexible, graph-based framework for agent construction and execution. LangGraph is positioned to influence the direction of artificial intelligence significantly in the future.\"}]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "LangGraph is an open-source AI agent framework created by LangChain, designed to build, deploy, and manage complex generative AI agent workflows. It provides tools and libraries that enable users to create, run, and optimize large language models (LLMs) efficiently and at scale. LangGraph uses graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow.\n",
      "\n",
      "Key features of LangGraph include:\n",
      "\n",
      "- **Agent Systems:** It provides a framework for building agent-based systems, useful in applications like robotics, autonomous vehicles, or video games.\n",
      "- **LLM Applications:** It enables developers to build sophisticated AI models that learn and improve over time.\n",
      "- **Graph-Based Architecture:** This architecture allows users to scale AI workflows without sacrificing efficiency, using enhanced decision-making by modeling complex relationships between nodes.\n",
      "\n",
      "LangGraph allows for more variable and nuanced agent behaviors compared to linear execution models, making it a significant advancement in AI agent development. It is designed to influence the future direction of artificial intelligence by offering a flexible, graph-based framework for agent construction and execution.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Answer:\n",
       "LangGraph is an open-source AI agent framework created by LangChain, designed to build, deploy, and manage complex generative AI agent workflows. It provides tools and libraries that enable users to create, run, and optimize large language models (LLMs) efficiently and at scale. LangGraph uses graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow.\n",
       "\n",
       "Key features of LangGraph include:\n",
       "\n",
       "- **Agent Systems:** It provides a framework for building agent-based systems, useful in applications like robotics, autonomous vehicles, or video games.\n",
       "- **LLM Applications:** It enables developers to build sophisticated AI models that learn and improve over time.\n",
       "- **Graph-Based Architecture:** This architecture allows users to scale AI workflows without sacrificing efficiency, using enhanced decision-making by modeling complex relationships between nodes.\n",
       "\n",
       "LangGraph allows for more variable and nuanced agent behaviors compared to linear execution models, making it a significant advancement in AI agent development. It is designed to influence the future direction of artificial intelligence by offering a flexible, graph-based framework for agent construction and execution."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query that triggers the Tavily search tool\n",
    "process_query(\"What is LangGraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "When is the next olympics?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_current_date (call_6ReT1NDyKkR91Eae1Rb0WHJG)\n",
      " Call ID: call_6ReT1NDyKkR91Eae1Rb0WHJG\n",
      "  Args:\n",
      "  tavily_search_results_json (call_RKcayrngZyCVnKjejSIv9ynB)\n",
      " Call ID: call_RKcayrngZyCVnKjejSIv9ynB\n",
      "  Args:\n",
      "    query: next Olympics date\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"url\": \"https://www.nbcolympics.com/news/when-are-next-winter-olympics-milan-cortina-2026-dates-venues-and-more\", \"content\": \"## When are the next Olympics?\\n\\nThe next Olympics will be the 2026 Milan Cortina Olympics. The Winter Olympics will take place from February 6 to February 22, with the Paralympics following from March 6 to March 15.\\n\\n## Where are the next Olympics?\"}, {\"url\": \"https://www.nbcolympics.com/news/where-and-when-are-next-olympics-la-2028-dates-venues-and-more\", \"content\": \"Where and when are the next Olympics? LA 2028 dates, venues and more | NBC OlympicsSkip to main content\\n\\nLeo Santos\\n\\nSep. 13, 2024 2:45 pm ET\\n\\n6m 17s\\n\\nClosing Ceremony\\n\\n[Tom Cruise delivers Olympic Flag from Paris to L.A.](/videos/tom-cruise-delivers-olympic-flag-paris-la)\\n\\nPLAY\\n\\nPlay Tom Cruise delivers Olympic Flag from Paris to L.A. video\\n\\nThe next Olympics will be held in Los Angeles from Friday, July 14 to Sunday, July 30, 2028.\"}]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The next Olympics will be the 2026 Winter Olympics, taking place in Milan Cortina from February 6 to February 22, 2026. The Paralympics will follow from March 6 to March 15, 2026. \n",
      "\n",
      "After that, the Summer Olympics will be held in Los Angeles from July 14 to July 30, 2028.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Answer:\n",
       "The next Olympics will be the 2026 Winter Olympics, taking place in Milan Cortina from February 6 to February 22, 2026. The Paralympics will follow from March 6 to March 15, 2026. \n",
       "\n",
       "After that, the Summer Olympics will be held in Los Angeles from July 14 to July 30, 2028."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Time-based query about an event\n",
    "process_query(\"When is the next olympics?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need for Memory\n",
    "\n",
    "Taking a look at an example of where the current chatbot falls short when remembering previous interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi. My name is Sajal. Who are you?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Sajal! I'm an AI assistant here to help you with information and answer your questions. How can I assist you today?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Answer:\n",
       "Hello Sajal! I'm an AI assistant here to help you with information and answer your questions. How can I assist you today?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "process_query(\"Hi. My name is Sajal. Who are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm sorry, but I don't have access to personal data about you, including your name.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Answer:\n",
       "I'm sorry, but I don't have access to personal data about you, including your name."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "process_query(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Memory to the Chatbot\n",
    "To maintain conversation state across calls, we use LangGraph's checkpointing system.\n",
    "\n",
    "#### Setting Up Checkpointing\n",
    "We create a MemorySaver checkpointer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the graph with the checkpointer\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Thread IDs for Conversation State\n",
    "We can maintain separate conversation threads by providing a thread_id in the configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi. My name is Sajal. Who are you?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Sajal! I'm an AI language model here to assist you with information, answer questions, and help with various tasks. How can I assist you today?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Answer:\n",
       "Hello Sajal! I'm an AI language model here to assist you with information, answer questions, and help with various tasks. How can I assist you today?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config_user_1 = {\"thread_id\": \"user_1\"}\n",
    "process_query(\"Hi. My name is Sajal. Who are you?\", config=config_user_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Sajal.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Answer:\n",
       "Your name is Sajal."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "process_query(\"What is my name?\", config=config_user_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm sorry, but I don't have access to personal data about individuals unless it has been shared with me in the course of our conversation. Therefore, I don't know your name. If you'd like, you can tell me your name or any other details you'd like to share.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Answer:\n",
       "I'm sorry, but I don't have access to personal data about individuals unless it has been shared with me in the course of our conversation. Therefore, I don't know your name. If you'd like, you can tell me your name or any other details you'd like to share."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking if memory is shared across threads\n",
    "config_user_2 = {\"thread_id\": \"user_2\"}\n",
    "process_query(\"What is my name?\", config=config_user_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "\n",
    "Congratulations on building your own chatbot agent from scratch with LangGraph. Be sure to go through the next few tutorials to learn more about adding human in the loop capabilities to this agent!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
